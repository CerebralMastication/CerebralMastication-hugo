<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.51" />


<title>Loading Big (ish) Data into R - A Hugo website</title>
<meta property="og:title" content="Loading Big (ish) Data into R - A Hugo website">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/cerebralmastication/">GitHub</a></li>
    
    <li><a href="https://twitter.com/CMastication">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">3 min read</span>
    

    <h1 class="article-title">Loading Big (ish) Data into R</h1>

    
    <span class="article-date">2009/11/24</span>
    

    <div class="article-content">
      <p>So for the rest of this conversation big data == 2 Gigs. Done. Don&rsquo;t give me any of this &lsquo;that&rsquo;s not big, THIS is big&rsquo; shit. There now, on with the cool stuff:</p>

<p>This week on twitter Vince Buffalo asked about loading a 2 gig comma separated file (csv) into R (OK, he asked about tab delimited data, but I ignored that because I use mostly comma data and I wanted to test CSV. Sue me.)</p>

<p><a href="http://twitter.com/vsbuffalo/statuses/5987999475"><img src="https://www.cerebralmastication.com/wp-content/uploads/2009/11/2gib.PNG" alt="2gib" />
</a></p>

<p>I thought this was a dang good question. What I have always done in the past was load my data into SQL Server or Oracle using an ETL tool and then suck it from the database to R using either native database connections or the RODBC package. <a href="http://twitter.com/mpastell/statuses/6002853376">Matti Pastell (@mpastell) recommended </a>using the <a href="http://code.google.com/p/sqldf/">sqldf </a>(SQL to data frame) package to do the import. I&rsquo;ve used sqldf before, but only to allow me to use SQL syntax to manipulate R data frames. I didn&rsquo;t know it could import data, but that makes sense, given how sqldf works. How does it work? Well sqldf sets up an instance of the <a href="http://www.sqlite.org/">sqlite </a>database server then shoves R data into the DB, does operations on the tables, and then spits out an R data frame of the results. What I didn&rsquo;t realize is that we can call sqldf from within R and have it import a text file directly into sqlite and then return the data from sqlite directly into R using a pretty fast native connection. I did a little Googling and came up with <a href="http://old.nabble.com/Re%3A-Memory-Experimentation%3A-Rule-of-Thumb-%3D-10-15-Times-the-Memory-to12076668.html#a12078165">this discussion </a>on the R mailing list.</p>

<p>So enough background, here&rsquo;s my setup: I have a Ubuntu virtual machine running with 2 cores and 10 gigs of memory. Here&rsquo;s the code I ran to test:</p>

<blockquote>bigdf <- data.frame(dim=sample(letters, replace=T, 4e7), fact1=rnorm(4e7), fact2=rnorm(4e7, 20, 50))
write.csv(bigdf, 'bigdf.csv', quote = F)</blockquote>

<p>That code creates a data frame with 3 columns. I created a single letter text column, then two floating point columns. There are 40,000,000 records. When I run the write.csv step on my machine I get about 1.8GiB. That&rsquo;s close enough to 2 gigs for me. I created the text file and then ran rm(list=ls()) to kill all objects. I then ran gc() and saw that I had hundreds of megs of something or other (I have not invested the brain cycles to understand the output that gc() gives). So I just killed and restarted R. I then ran the following:</p>

<blockquote>library(sqldf)
f <- file("bigdf.csv")
system.time(bigdf <- sqldf("select * from f", dbname = tempfile(), file.format = list(header = T, row.names = F)))</blockquote>

<p>That code loads the CSV into an sqlite DB then executes a select * query and returns the results to the R data frame bigdf. Pretty straightforward, ey? Well except for the dbname = tempfile() bit. In sqldf you can choose where it makes the sqlite db. If you don&rsquo;t specify at all it makes it in memory which is what I first tried. I ran out of mem even on my 10GB box. So I read a little more and added the dbname = tempfile() which creates a temporary sqlite file on the disk. If I wanted to use an existing sqlite file I could have specified that instead.</p>

<p>So how long did it take to run? Just under 5 minutes.</p>

<p>So how long would the read.csv method take? Funny you should ask. I ran the following code to compare:</p>

<blockquote>system.time(big.df <- read.csv('bigdf.csv'))</blockquote>

<p>And I would love to tell you how long that took to run, but it&rsquo;s been running for half an hour all night and I just don&rsquo;t have that kind of patience.</p>

<p>-JD</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    

    
  </body>
</html>

