<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.51" />


<title>Principal Component Analysis (PCA) vs Ordinary Least Squares (OLS): A Visual Explanation - A Hugo website</title>
<meta property="og:title" content="Principal Component Analysis (PCA) vs Ordinary Least Squares (OLS): A Visual Explanation - A Hugo website">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/cerebralmastication/">GitHub</a></li>
    
    <li><a href="https://twitter.com/CMastication">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">3 min read</span>
    

    <h1 class="article-title">Principal Component Analysis (PCA) vs Ordinary Least Squares (OLS): A Visual Explanation</h1>

    
    <span class="article-date">2010/09/16</span>
    

    <div class="article-content">
      <p><a href="https://www.cerebralmastication.com/wp-content/uploads/2010/09/sa.png"><img src="https://www.cerebralmastication.com/wp-content/uploads/2010/09/sa.png" alt="" />
</a>Over at stats.stackexchange.com recently, a <a href="http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/2700#2700">really interesting question was raised</a> about principal component analysis (PCA). The gist was &ldquo;Thanks to my college class I can do the math, but what does it <strong>MEAN</strong>?&rdquo;</p>

<p>I felt like this a number of times in my life. Many of my classes were focused on the technical implementations they kinda missed the section titled &ldquo;Why I give a shit.&rdquo; A perfect example was my Mathematics Principles of Economics class which taught me how to manually calculate a bordered Hessian but, for the life of me, I have no idea why I would ever want to calculate such a monster.  OK, that&rsquo;s a lie. Later in life I learned that bordered Hessian matrices are a second derivative test used in some optimizations. Not that I would EVER do that shit by hand. I&rsquo;d use some R package and blindly trust that it was coded properly.</p>

<p>So back to PCA: as I was reading the aforementioned stats question I was reminded of a recent presentation that <a href="http://quanttrader.info/public/">Paul Teetor</a> gave at a August Chicago R User Group. In his presentation on spread trading with R he showed a graphic that illustrated the difference between OLS and PCA. I took some notes and went home and made sure I could recreate the same thing. If you have wondered what makes OLS and PCA different, open up an R session and play along.</p>

<p><strong>Your Independent Variable Matters:</strong></p>

<p>The first observation to make is that regressing x ~ y is not the same as y ~ x even in a simple univariate regression. You can illustrate this by doing the following:</p>

<blockquote>set.seed(2)
x <- 1:100

y <- 20 + 3 * x
e <- rnorm(100, 0, 60)
y <- 20 + 3 * x + e

plot(x,y)
yx.lm <- lm(y ~ x)
lines(x, predict(yx.lm), col="red")

xy.lm <- lm(x ~ y)
lines(predict(xy.lm), y, col="blue")</blockquote>

<p>You should get something that looks like this:</p>

<p><a href="https://www.cerebralmastication.com/wp-content/uploads/2010/09/olsVSols.png"><img src="https://www.cerebralmastication.com/wp-content/uploads/2010/09/olsVSols-280x300.png" alt="" />
</a></p>

<p>So it&rsquo;s obvious they give different lines. But why? Well, OLS minimizes the error between the dependent and the model. Two of these errors are illustrated for the y ~ x case in the following picture:</p>

<p><a href="https://www.cerebralmastication.com/wp-content/uploads/2010/09/OLS1.png"><img src="https://www.cerebralmastication.com/wp-content/uploads/2010/09/OLS1-280x300.png" alt="" />
</a></p>

<p>But when we flip the model around and regress x ~ y then OLS minimizes these errors:</p>

<p><a href="https://www.cerebralmastication.com/wp-content/uploads/2010/09/OLS2.png"><img src="https://www.cerebralmastication.com/wp-content/uploads/2010/09/OLS2-280x300.png" alt="" />
</a></p>

<p>Ok, so what about PCA?</p>

<p>Well let&rsquo;s draw the first principal component the old school way:</p>

<blockquote>#normalize means and cbind together
xyNorm <- cbind(x=x-mean(x), y=y-mean(y))
plot(xyNorm)

#covariance
xyCov <- cov(xyNorm)
eigenValues <- eigen(xyCov)$values
eigenVectors <- eigen(xyCov)$vectors

plot(xyNorm, ylim=c(-200,200), xlim=c(-200,200))
lines(xyNorm[x], eigenVectors[2,1]/eigenVectors[1,1] * xyNorm[x])
lines(xyNorm[x], eigenVectors[2,2]/eigenVectors[1,2] * xyNorm[x])

# the largest eigenValue is the first one
# so that's our principal component.
# but the principal component is in normalized terms (mean=0)
# and we want it back in real terms like our starting data
# so let's denormalize it
plot(xy)
lines(x, (eigenVectors[2,1]/eigenVectors[1,1] * xyNorm[x]) + mean(y))
# that looks right. line through the middle as expected

# what if we bring back our other two regressions?
lines(x, predict(yx.lm), col="red")
lines(predict(xy.lm), y, col="blue")</blockquote>

<p>PCA minimizes the error orthogonal (perpendicular) to the model line. So first principal component  looks like this:</p>

<p><a href="https://www.cerebralmastication.com/wp-content/uploads/2010/09/pca.png"><img src="https://www.cerebralmastication.com/wp-content/uploads/2010/09/pca-280x300.png" alt="" />
</a></p>

<p>The two yellow lines, as in the previous images, examples of two of the errors which the routine minimizes.</p>

<p>So if we plot all three lines on the same scatter plot we can see the differences:</p>

<p><a href="https://www.cerebralmastication.com/wp-content/uploads/2010/09/olsVSpca.png"><img src="https://www.cerebralmastication.com/wp-content/uploads/2010/09/olsVSpca-280x300.png" alt="" />
</a></p>

<p>The x ~ y OLS and the first principal component are pretty close, but click on the image to get a better view and you will see they are not exactly the same.</p>

<p>All the code from the above examples can be found in a <a href="http://gist.github.com/582767">gist over at GitHub.com</a>. It&rsquo;s best to copy and past from the github as sometimes Wordpress molests my quotes and breaks the codez.</p>

<p>The best introduction to PCA which I have read is the one I link to on Stats.StackExchange.com. It&rsquo;s titled <a href="http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf">&ldquo;A Tutorial on Principal Components Analysis&rdquo; by Lindsay I Smith</a>.</p>

    </div>
  </article>

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//cerebralmastication.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>



</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-129487349-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>

